---
title: "Honors Thesis Progress"
author: "Joel Ramirez Jr"
date: "7/1/2020"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
library(readxl)
library(ppcor)
library(gridExtra)
```



```{r, echo = FALSE, message = FALSE}
hourlyOrdered16 <- read_csv("TL3Data/hourlyOrdredAwc16.csv") 
hourlyProp16 <- read_csv("TL3Data/hourlyProportions.csv")
segOrdered16 <- read_csv("TL3Data/segOrdredAwc16.csv") 
segProp16 <- read_csv("TL3Data/segProportions.csv") 
distributionPerHour16 <- read_csv("TL3Data/distributionPerHour16.csv")
distributionPerSegment16 <- read_csv("TL3Data/distributionPerSegment16.csv")
distributionSumation16 <- read_csv("TL3Data/distributionSumation16.csv")
sotCodingSheet16 <- read_csv("TL3Data/cleanedSotCodingSheet16.csv") 
clusterValuesSize2 <- read_csv("clusterMeasuresMin2.csv")
TL3MetadataRenamed <- read_csv("TL3Data/TL3data_Renamed.csv")
```



```{r, echo = FALSE, message = FALSE}
child_dist_per_hour.ordered <- function(df, na.rm = TRUE, ...){
  
  # create list of counties in data to loop over 
  subject_list <- unique(distributionPerHour16$SUBJECT)
  # create for loop to produce ggplot2 graphs 
  for (i in seq_along(subject_list)) { 
    
    # create plot for each subject in df 
    cur_df <- df %>% 
      filter(SUBJECT == subject_list[i])
    
    plot <- 
      ggplot(cur_df, aes( x = as.numeric(reorder(hour_bin, clean_total_awc)),
                          y = as.numeric(clean_total_awc) )) +
      ylim(0.0, 7000.0) +
    geom_bar(stat = "identity",position = "dodge") +
      theme_gray() +
      ggtitle(paste( "Subject", subject_list[i], 'Hourly Caregiver Talk',
                    sep=''))
    # save plots as .png
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".png", sep=''), scale=2)
   
    # save plots as .pdf
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".pdf", sep=''), scale=2)
    
    # print plots to screen
    print(plot)
  }
}

child_dist_per_hour.unordered <- function(df, na.rm = TRUE, ...){
  
  # create list of counties in data to loop over 
  subject_list <- unique(distributionPerHour16$SUBJECT)
  # create for loop to produce ggplot2 graphs 
  for (i in seq_along(subject_list)) { 
    
    # create plot for each subject in df 
    cur_df <- df %>% 
      filter(SUBJECT == subject_list[i])
    
    plot <- 
      ggplot(cur_df, aes( x = hour_bin,
                          y = clean_total_awc)) +
      ylim(0.0, 7000.0) +
    geom_bar(stat = "identity",position = "dodge") +
      theme_gray() +
      ggtitle(paste( "Subject", subject_list[i], 'Hourly Caregiver Talk',
                    sep=''))
    # save plots as .png
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".png", sep=''), scale=2)
   
    # save plots as .pdf
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".pdf", sep=''), scale=2)
    
    # print plots to screen
    print(plot)
  }
}

child_dist_per_seg.ordered <- function(df, na.rm = TRUE, ...){
  
  # create list of counties in data to loop over 
  subject_list <- unique(distributionPerHour16$SUBJECT)
  # create for loop to produce ggplot2 graphs 
  for (i in seq_along(subject_list)) { 
    
    # create plot for each subject in df 
    
    cur_df <- df %>% 
      filter(SUBJECT == subject_list[i])
    
    plot <- 
      ggplot(cur_df, aes( x = as.numeric(reorder(Time, AWC)),
                          y = as.numeric(AWC) )) +
    geom_bar(stat = "identity",position = "dodge") +
      theme_gray() +
      ggtitle(paste( "Subject ", subject_list[i], ' Per Segment Caregiver Talk',
                    sep=''))
    # save plots as .png
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".png", sep=''), scale=2)
   
    # save plots as .pdf
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".pdf", sep=''), scale=2)
    
    # print plots to screen
    print(plot)
  }
}

child_dist_per_seg.unordered <- function(df, na.rm = TRUE, ...){
  
  # create list of counties in data to loop over 
  subject_list <- unique(distributionPerHour16$SUBJECT)
  # create for loop to produce ggplot2 graphs 
  for (i in seq_along(subject_list)) { 
    
    # create plot for each subject in df 
    cur_df <- df %>% 
      filter(SUBJECT == subject_list[i])
    
    plot <- 
      ggplot(cur_df, aes( x = as.numeric(Time),
                          y = as.numeric(AWC) )) +
      ylim(0.0, 500.0) +
    geom_bar(stat = "identity",position = "dodge") +
      theme_gray() +
      ggtitle(paste( "Subject ", subject_list[i], ' Per Segment Caregiver Talk',
                    sep=''))
    # save plots as .png
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".png", sep=''), scale=2)
   
    # save plots as .pdf
    # ggsave(plot, file=paste(results, 
    #                        'projection_graphs/county_graphs/',
    #                        county_list[i], ".pdf", sep=''), scale=2)
    
    # print plots to screen
    print(plot)
  }
}

```


# A Day in the Life of a Child...(on average)

A childs day to day unordered AWC profile would look something like this. Where time is in military time just for the simplicity in plotting. We do see that it is very variable and also has a large window of talk, starting from 8 am to about 7pm. With AWC ranging from 0 words to 667 words within a 5 minute window. 

```{r, echo = FALSE, message = FALSE}  
subjectExample <- sotCodingSheet16 %>% 
  filter(SUBJECT == 11003)

ggplot(subjectExample, aes( x = Time, y = AWC )) +
    geom_bar(stat = "identity",position = "dodge") +
      theme_gray() +
      ggtitle("Subject 11003 Profile of Segment Caregiver Talk")


```



```{r, echo = FALSE, message = FALSE}
talkFrequency <- distributionSumation16 %>% 
  group_by(hour_bin) %>% 
   mutate(unclean_avg_awc_across_all_subjects = mean(unclean_total_awc),
         unclean_avg_cvc_across_all_subjects = mean(unclean_total_cvc),
         unclean_avg_ctc_across_all_subjects = mean(unclean_total_ctc)) %>% 
  
  mutate(clean_avg_awc_across_all_subjects = mean(clean_total_awc),
         clean_avg_cvc_across_all_subjects = mean(clean_total_cvc),
         clean_avg_ctc_across_all_subjects = mean(clean_total_ctc)) %>% 
  distinct(hour_bin, unclean_avg_awc_across_all_subjects, unclean_avg_cvc_across_all_subjects,
           unclean_avg_ctc_across_all_subjects, clean_avg_awc_across_all_subjects, clean_avg_cvc_across_all_subjects
           ,clean_avg_ctc_across_all_subjects )
```


# So, where is talk happening?
First we have the unclean LENA histogram across all subjects.
Observations:

- There are two big dense periods throughout the day
- These periods are in the morning and in the evening, maybe due to parents coming and going from work?


```{r, echo = FALSE, message = FALSE}
ggplot(talkFrequency, aes(x = hour_bin,y = unclean_avg_awc_across_all_subjects)) +
    geom_bar(stat = "identity",position = "dodge") +
  theme_gray()


# ggplot(talkFrequency, aes(x = hour_bin,y = unclean_avg_ctc_across_all_subjects)) +
#     geom_bar(stat = "identity",position = "dodge") +
#   theme_gray()
# 
# ggplot(talkFrequency, aes(x = hour_bin,y = unclean_avg_cvc_across_all_subjects)) +
#     geom_bar(stat = "identity",position = "dodge") +
#   theme_gray()
```
## Cleaning for Naps

Cleaning for naps is basically getting rid of all instances where the child is sleeping.
This decision was made as for the two samples which we are comparing, the Spanish group turned off the recorders when the child slept or sometimes for some window within the day. So, to make it comparible we get rid of instances when children are sleeping and/or napping. 

# Mean AWC with Standard Deviation of Mean AWC

The first plot is unclean mean awc X unclean standard deviation. These are measures for AWC without cleaning for naps.

The second plot is clean mean awc X clean standard deviation.

```{r, echo = FALSE, message = FALSE}
ggplot(distributionPerHour16, aes(unclean_mean_awc, unclean_sd_awc))+
  geom_point() +
   geom_smooth(method = "lm")

ggplot(distributionPerHour16, aes(clean_mean_awc, clean_sd_awc))+
  geom_point() +
   geom_smooth(method = "lm")
```





```{r, echo = FALSE, message = FALSE}
# # Plotting mean CTC with Std Dev CTC
# The first plot is unclean mean ctc X unclean ctc standard deviation. This are measures for ctc without cleaning for naps.
# The second plot is clean mean ctc X clean ctc standard deviation.
# #Then, we will plot each, but first we will need to reshape the data frame into a long format
# ggplot(distributionPerSegment16, aes(unclean_mean_ctc, unclean_sd_ctc))+
#   geom_point()+
#    geom_smooth(method = "lm")
# 
# ggplot(distributionPerSegment16, aes(clean_mean_ctc, clean_sd_ctc))+
#   geom_point()+
#    geom_smooth(method = "lm")
```


```{r, echo = FALSE, message = FALSE}
## Plotting mean CVC with Std Dev CVC
# 
# The first plot is unclean mean CVC X unclean CVC standard deviation. This are measures for CVC without cleaning for naps.
# 
# The second plot is clean mean CVC X clean CVC standard deviation.
# #Then, we will plot each, but first we will need to reshape the data frame into a long format
# ggplot(distributionPerSegment16, aes(unclean_mean_cvc, unclean_sd_cvc))+
#   geom_point()+
#    geom_smooth(method = "lm")
# 
# ggplot(distributionPerSegment16, aes(clean_mean_cvc, clean_sd_cvc))+
#   geom_point()+
#    geom_smooth(method = "lm")
```



```{r, echo = FALSE, message = FALSE}
#Then, we will plot each, but first we will need to reshape the data frame into a long format
# ggplot(distributionPerHour16, aes(unclean_mean_awc, unclean_sd_awc))+
#   geom_point()+
#    geom_smooth(method = "lm")
# 
# ggplot(distributionPerHour16, aes(clean_mean_awc, clean_sd_awc))+
#   geom_point()+
#    geom_smooth(method = "lm")
```

```{r, echo = FALSE, message = FALSE }
# #Then, we will plot each, but first we will need to reshape the data frame into a long format
# ggplot(distributionPerHour16, aes(unclean_mean_ctc, unclean_sd_ctc))+
#   geom_point()
# 
# ggplot(distributionPerHour16, aes(clean_mean_ctc, clean_sd_ctc))+
#   geom_point()
```

```{r, echo = FALSE, message = FALSE }
#Then, we will plot each, but first we will need to reshape the data frame into a long format
# ggplot(distributionPerHour16, aes(unclean_mean_cvc, unclean_sd_cvc))+
#   geom_point()
# 
# ggplot(distributionPerHour16, aes(clean_mean_cvc, clean_sd_cvc))+
#   geom_point()'
```



```{r, echo = FALSE, message = FALSE}
# Next we have the clean LENA histograms across all subjects. We have AWC, CTC, and CVC. 
# Observations:
# 
# - Similarly, here are two big dense periods throughout the day
# - These periods are in the morning and in the evening, maybe due to parents coming and going from work.
# ggplot(talkFrequency, aes(x = hour_bin,y = clean_avg_awc_across_all_subjects)) +
#     geom_bar(stat = "identity",position = "dodge") +
#   theme_gray()
# 
# ggplot(talkFrequency, aes(x = hour_bin,y = clean_avg_ctc_across_all_subjects)) +
#     geom_bar(stat = "identity",position = "dodge") +
#   theme_gray()
# 
# ggplot(talkFrequency, aes(x = hour_bin,y = clean_avg_cvc_across_all_subjects)) +
#     geom_bar(stat = "identity",position = "dodge") +
#   theme_gray()
```




```{r, echo = FALSE, message = FALSE}
# child_dist_per_hour.ordered(distributionSumation16)
## what is the difference between top hour and bottom hour? and what is the mean/proportion maximal difference between hours. 



```

```{r, echo = FALSE, message = FALSE}
# child_dist_per_hour.unordered(distributionSumation16)
## what is the difference between top hour and bottom hour? and what is the mean/proportion maximal difference between hours. 
```


```{r, echo = FALSE, message = FALSE}
# child_dist_per_seg.ordered(sotCodingSheet16)

## fix the squigily graph

# is there a relationship between how steep your curve is and the avereage,
# Do steeper people have a higher average overall?
# You have people who are bursty, if you are bursty, do you have a higher average overall or can we decouple burstyness? 
# Are there pockets of time while the rest is not as intense.
# Can you be low overall but still be bursty?
# Average, steepness, and burstyness. 

# child_dist_per_seg.unordered(sotCodingSheet16)

```

```{r, echo = FALSE, message = FALSE}
# child_dist_per_seg.unordered(sotCodingSheet16)

## fix the squigily graph

# is there a relationship between how steep your curve is and the avereage,
# Do steeper people have a higher average overall?
# You have people who are bursty, if you are bursty, do you have a higher average overall or can we decouple burstyness? 
# Are there pockets of time while the rest is not as intense.
# Can you be low overall but still be bursty?


# Average, steepness, and burstyness. 

```

# Proportional Drop
Proportional drop was a measure we came up with to measure variance in the child's moment-to-moment language environment.
In a list of AWCs, orderd from smalled to largest where $AWC_i \geq AWC_{i=1}$, for a single child, proportional drop for two single observations can be seen as $P\Delta = 1 - AWC_{i-1}/AWC_i$ where $AWC_{i-1}/AWC_i$ is the percentage of AWC from one drop to the other. And, so to see the proportional drop we take $1 - AWC_{i-1}/AWC_i$ to get the proportional difference between the $i^{th}$ and $i^{th}-1$. 

We used two measures:

- average proportional drop across the day
- max drop throughout the day

Observations: 
  We saw that every child had at least two contiguous segments with the same AWC count. Thus, all children had a mininum drop of 0.
  

```{r, echo = FALSE, message = FALSE}
#Loading in overall proportional differnce.
subjectPropDrop <- segProp16 %>% 
  group_by(SUBJECT) %>% 
  mutate(propDrop = 1-prop) %>% 
  mutate(avgPropDrop = mean(propDrop), maxPropDrop = max(propDrop), minPropDrop = min(propDrop)) %>% 
  distinct(avgPropDrop, maxPropDrop, minPropDrop)

segmentalDist16 <- merge(subjectPropDrop, distributionPerSegment16, by = 'SUBJECT')
```

Here we see the histogram proportional drop across the entire day as well as a summary of its statistics.

```{r, echo = FALSE, message = FALSE}
hist(subjectPropDrop$avgPropDrop)
summary(subjectPropDrop$avgPropDrop)
```

Here are side by side graphs of average proportional drop against clean mean AWC and standard deviation AWC.

```{r, echo = FALSE, message = FALSE}
pdXcma <- ggplot(segmentalDist16, aes(x = avgPropDrop, y = clean_mean_awc))+
  geom_point() + 
  geom_smooth(method = "lm")

pdXcsda <- ggplot(segmentalDist16, aes(x = avgPropDrop, y = clean_sd_awc))+
  geom_point() + 
  geom_smooth(method = "lm")

grid.arrange(pdXcma, pdXcsda, ncol=2)

```

Here are side by side graphs of max proportional drop against clean mean AWC and standard deviation AWC.
We see that they are not correlated at all and so at this point we decided to leave the measure alone.

```{r, echo = FALSE, message = FALSE}
mxpXcma <- ggplot(segmentalDist16, aes(x = maxPropDrop, y = clean_mean_awc))+
  geom_point() + 
  geom_smooth(method = "lm")

mxpXcsda <- ggplot(segmentalDist16, aes(x = maxPropDrop, y = clean_sd_awc))+
  geom_point() + 
  geom_smooth(method = "lm")


grid.arrange(mxpXcma, mxpXcsda, ncol=2)

```



```{r, echo = FALSE, message = FALSE}

#some people can be steepy and low and some people can be steepy and high
#some people can be grad and low and some people can be grad and high
#What does all day AWC mean?? High awc means ppl talk all the time??? nah, it's just an average accross the day
#people are variable from moment to moment and talk about the shape of the overall dist, with bursty and gradual people
#some people have a biggere difference on average between each 5 minute segment
#DREW ABNEY (LITERATURE ON BURSTYNESS)
#START A TIMELINE FOR DEADLINES
#SRCD BI ANNUAL CONFERENCE ABOUT FAMILY MATTERS 
#start back, from the start with AWC and what it means.
#then get to the peakyness and awc 
#go back to the casillas paper, with how can people learn from nothing? 
#the nothing is defined from very low talk (on average)
#are there peaks of intense language?

#where we are at is not an easy place to be at, so use some graphics and make it easier to understand!cisc
#Then, we will plot each, but first we will need to reshape the data frame into a long format

# 
# ggplot(distributionPerSegment16, aes(unclean_mean_ctc, unclean_sd_ctc))+
#   geom_point()
# 
# ggplot(distributionPerSegment16, aes(clean_mean_ctc, clean_sd_ctc))+
#   geom_point()
# 
# ggplot(distributionPerSegment16, aes(x = reorder(SUBJECT, clean_mean_ctc),y = clean_mean_ctc)) +
#     geom_bar(stat = "identity",position = "dodge") +
#   geom_errorbar(width=.1, aes(ymin=clean_mean_ctc-clean_sd_ctc, ymax=clean_mean_ctc+clean_sd_ctc)) +
#   theme_gray()
# 
# english <- sotCodingSheet16 %>% 
#   replace_na(list(Exclude = "keep")) %>% 
#   replace_na(list(DNL = "0")) %>% 
#   replace_na(list(Sleep = "0"))
# 
# date_seq <- english %>% 
#   group_by(SUBJECT, Date) %>% 
#   distinct(Date) %>% 
#   group_by(SUBJECT) %>% 
#   arrange(Date) %>% 
#   mutate(date_seq = seq_len(n()))
# 
# english2 <- english %>% 
#   full_join(date_seq, by = c("SUBJECT", "Date"))
# 
# 
# hours_rec_uncleaned_full_recording <- english2 %>% 
#   group_by(SUBJECT) %>% 
#   mutate(hours_rec_uncleaned_full_recording = sum(Duration)/3600) %>% 
#   distinct(SUBJECT, hours_rec_uncleaned_full_recording)
# 
# ggplot(hours_rec_uncleaned_full_recording, aes(hours_rec_uncleaned_full_recording)) + 
#   geom_histogram() 
# 
# english <- english %>% 
#  filter(SUBJECT < 911000)
# 
# ggplot(english, aes(Time, SUBJECT)) +
#   geom_point()
```

# Clustering Measures

As we saw before, the language landscape of a child is very variable throughout the day. So how to we simplify this? 

```{r, echo = FALSE, message = FALSE}  
subjectExample <- sotCodingSheet16 %>% 
  filter(SUBJECT == 11003)

ggplot(subjectExample, aes( x = Time, y = AWC )) +
    geom_bar(stat = "identity",position = "dodge") +
      theme_gray() +
      ggtitle("Subject 11003 Profile of Segment Caregiver Talk")
```


To find periods of dense talk, we identified the highest 30% of AWC segments for each family, and then applied a k-means clustering algorithm to identify one or more consecutive 5 minute segments that were similar in AWC (i.e., clusters of talk).

```{r, echo = FALSE, message = FALSE}
subjectExample <- sotCodingSheet16 %>% 
  filter(SUBJECT == 11003) %>% 
  filter(AWC >= quantile(sotCodingSheet16$AWC, probs = c(.7)))

ggplot(subjectExample, aes( x = Time, y = AWC )) +
    geom_bar(stat = "identity",position = "dodge") +
      theme_gray() +
      ggtitle("Subject 11003 70th Percentile Profile of Segment Caregiver Talk")

```

Thus, it simplifies the work we have to do by only looking at the top 30th percentile of AWCs for each child and also helps us better see the amounts of peaks a child has throughout the day. So when we run a kmeans custering algorithm, we run it on these top 30th percentile awcs for each child respectively.

# K-Means Clustering
k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. 

Given a set of observations $(x_1, x_2, ..., x_n)$, where each observation is a d-dimensional real vector, k-means clustering aims to partition the $n$ observations into $k (\leq n)$ sets $S = {S_1, S_2, ..., S_k}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:

${\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}={\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}|S_{i}|\operatorname {Var} S_{i}}$

Using the K-means clustering approach, we found these statistics.

- Range(0, 35) in the number of clusters found for each child. We are currently investigating why this number is so high.
- Mean number of of clusters is approximately 13.9.
- Average cluster Size (i.e. duration) was 2.07. This means that each cluster was approximately 10 minutes long.

To take into account differences in recording length, which had a range of 5 to 12 full hours (clean), we made the number of clusters a rate per hour. Current statistics are as follows:

- Mean: 1.413 clusters per hour.
- Median: 1.352 clusters per hour


Clusters per hour turned out to correlate significantly with Mean AWC and Standard Deviation of Average AWC. 


```{r, echo = FALSE, message = FALSE}

clusterAndDistrubtionMeasures <- merge(clusterValuesMin, distributionPerHour16, by = 'SUBJECT') 

clusterAndDistrubtionMeasures <- clusterAndDistrubtionMeasures %>% 
  mutate(clusterPerHr = nclusters/clean_total_hrs) 

clusterAndDistrubtionMeasures <- merge(clusterAndDistrubtionMeasures, TL3MetadataRenamed, by = 'SUBJECT')


clusterPerHourXMeanAWC <- ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_mean_awc))+
  geom_point() +
  geom_smooth(method = "lm")+
  labs(caption = "Cor: 0.6456; P<.001")

# cor.test(clusterAndDistrubtionMeasures$clusterPerHr, clusterAndDistrubtionMeasures$clean_mean_awc, method = c("pearson"))

clusterPerHourXSTDAWC <- ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_sd_awc))+
  geom_point() +
  geom_smooth(method = "lm")+
  labs(caption = "Cor: 0.5149; P<.001") 

# cor.test(clusterAndDistrubtionMeasures$clusterPerHr, clusterAndDistrubtionMeasures$clean_sd_awc, method = c("pearson"))

grid.arrange(clusterPerHourXMeanAWC, clusterPerHourXSTDAWC, ncol=2)
```

Average cluster size was found not to correlate with AWC or number of clusters as we see below.

```{r, echo = FALSE, message = FALSE}

a <- ggplot(clusterAndDistrubtionMeasures, aes(avgclustersize, clean_mean_awc))+
  geom_point() +
  geom_smooth(method = "lm")

# cor.test(clusterAndDistrubtionMeasures$avgclustersize, clusterAndDistrubtionMeasures$clean_mean_awc, method = c("pearson"))


b <- ggplot(clusterAndDistrubtionMeasures, aes(avgclustersize, nclusters))+
  geom_point() +
  geom_smooth(method = "lm")
# cor.test(clusterAndDistrubtionMeasures$avgclustersize, clusterAndDistrubtionMeasures$clean_sd_awc, method = c("pearson"))


grid.arrange(a, b, ncol=2)
```


Next, we want to look at how these measures predict to later language outcomes. 

# Clusters Against Vocabulary Size, Reaction Time, and Accuracy 3 Months Later.

Children’s vocabulary size was assessed using the MacArthur-Bates Communicative Development Inventories (CDI) in English (Fenson et al. 2007) or Spanish (Jackson-Maldonado et al., 2003). This is a parent survey asking how many common words the child knows at 15 months. Children’s language processing efficiency was assessed in the Looking-While-Listening task (Fernald et al., 2006). Here, children are present with two pictures, a target and a distraction. Both are on opposite sides of a monitor. A voice says, 'Can you see the ____'. And so, the elapsed time it takes the child to look at the target picture from the distraction is the Reaction Time (RT) and Accuracy reflects the proportion of time fixating on the named picture. 


## First, we predict to all outcome measures from Clusters per Hour
```{r, echo = FALSE, message = FALSE}
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_mean_awc))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_sd_awc))+
#   geom_point() +
#    geom_smooth(method = "lm")

clusterXCDI <- ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, CDI_18C18C_WG))+
  geom_point() +
  geom_smooth(method = "lm") +
   ggtitle("Predicting to Vocabulary") +
  xlab("Clusters/Hour ~ 16 Months") + ylab("CDI at 18 Months")


clusterXACC <- ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, ACC_1818))+
  geom_point() +
   geom_smooth(method = "lm")+
  ggtitle("Predicting to Acc") +
  xlab("Clusters/Hour ~ 16 Months") + ylab("Acurracy at 18 Months")

clusterXRT <- ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, RT18A18C_NO1RT))+
  geom_point() +
   geom_smooth(method = "lm")+
  ggtitle("Predicting to RT") +
  xlab("Clusters/Hour ~ 16 Months") + ylab("Reaction Time at 18 Months")


grid.arrange(clusterXCDI,clusterXACC, clusterXRT, ncol=3)
```

## Next, we predict to all outcome measures from Average Word Count
```{r, echo = FALSE, message = FALSE}
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_mean_awc))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_sd_awc))+
#   geom_point() +
#    geom_smooth(method = "lm")

clusterXCDI <- ggplot(clusterAndDistrubtionMeasures, aes(clean_mean_awc, CDI_18C18C_WG))+
  geom_point() +
  geom_smooth(method = "lm") +
   ggtitle("Predicting to Vocabulary") +
  xlab("Mean AWC/Hour ~ 16 Months") + ylab("CDI at 18 Months")


clusterXACC <- ggplot(clusterAndDistrubtionMeasures, aes(clean_mean_awc, ACC_1818))+
  geom_point() +
   geom_smooth(method = "lm")+
  ggtitle("Predicting to Acc") +
  xlab("Mean AWC/Hour ~ 16 Months") + ylab("Acurracy at 18 Months")

clusterXRT <- ggplot(clusterAndDistrubtionMeasures, aes(clean_mean_awc, RT18A18C_NO1RT))+
  geom_point() +
   geom_smooth(method = "lm")+
  ggtitle("Predicting to RT") +
  xlab("Mean AWC/Hour ~ 16 Months") + ylab("Reaction Time at 18 Months")


grid.arrange(clusterXCDI,clusterXACC, clusterXRT, ncol=3)
```

From these graphs we see that clusters/hour was significantly correlated with Accuracy and CDI score (p<.01). Critically, clusters/hour accounted for an additional 15% of the variance in CDI scores, after controlling for AWC (p = 0.012). It is important to note though that Mean AWC per Hour account for the majority of the variance for both Accuracy and Reaction Time.

These results demonstrate that in addition to overall amount of talk, understanding dense periods of talk over the day provides a more nuanced view of the patterns of talk to young children. Importantly, the amount of dense talk provides additional predictive power when accounting for children's outcomes. Ongoing analyses of Spanish-speaking families will investigate if similar trends can be replicated in families from a different cultural and linguistic group. 

```{r, echo = FALSE, message = FALSE}

# #Then, we will plot each, but first we will need to reshape the data frame into a long format
# ggplot(clusterAndDistrubtionMeasures, aes(nclusters, clean_mean_awc))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# cor.test(clusterAndDistrubtionMeasures$nclusters, clusterAndDistrubtionMeasures$clean_mean_awc, method = c("pearson"))
# 
# 

# ggplot(clusterAndDistrubtionMeasures, aes(nclusters, clean_sd_awc))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(nclusters, clean_total_hrs))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# pcor.test(clusterAndDistrubtionMeasures$clean_mean_awc, clusterAndDistrubtionMeasures$avgclustersize, clusterAndDistrubtionMeasures$clean_total_hrs, method = c("pearson"))
# 
# ggplot(clusterAndDistrubtionMeasures, aes(avgclustersize, clean_mean_awc))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# 
# 
# ggplot(clusterAndDistrubtionMeasures, aes(avgclustersize, clean_sd_awc))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# 
# ggplot(clusterAndDistrubtionMeasures, aes(avgclustersize, clean_total_hrs))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# 
# 

# #Predicting to Mean AWC 
# model1 = lm(clean_mean_awc ~ nclusters, data = clusterAndDistrubtionMeasures)
# summary(model1)
# 
# model2 = lm(clean_mean_awc ~ nclusters + clean_total_hrs, data = clusterAndDistrubtionMeasures) 
# summary(model2)
# 
# model3 = lm(clean_mean_awc ~ avgclustersize + clean_total_hrs, data = clusterAndDistrubtionMeasures) 
# summary(model3)
# 
# 
# ##predicting to cdi
# model1 = lm(CDI_18C18C_WG ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model1)
# 
# model2 = lm(CDI_18C18C_WG ~ clean_mean_awc + clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model2)
# 
# 
# ##Predicting to RT 
# model4 = lm(RT18A18C_NO1RT ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model4)
# 
# model5 = lm(RT18A18C_NO1RT ~ clean_mean_awc + clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model5)
# 
# 
# #Predictin to ACC
# model6 = lm(ACC_1818 ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model6)
# 
# model7 = lm(ACC_1818 ~ clean_mean_awc + clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model7)
# 
# ## Predicting to CDI only Clean Mean AWC
# 
# model1 = lm(CDI_18C18C_WG ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model1)
# 
# model2 = lm(CDI_18C18C_WG ~ clean_mean_awc + clean_sd_awc, data = clusterAndDistrubtionMeasures)
# summary(model2)
# 
# ## Predicting to RT only Clean Mean AWC
# model4 = lm(RT18A18C_NO1RT ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model4)
# 
# model5 = lm(RT18A18C_NO1RT ~ clean_mean_awc + clean_sd_awc, data = clusterAndDistrubtionMeasures)
# summary(model5)
# 
# 
# ## Predicting to RT only Clean Mean AWC
# model6 = lm(ACC_1818 ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model6)
# 
# model7 = lm(ACC_1818 ~ clean_mean_awc + clean_sd_awc, data = clusterAndDistrubtionMeasures)
# summary(model7)
# 
# 
# 
# summary(model7)
# #using anova
# 
# anova(model1, model2)
# 
# ```
# ##the more times you pop up with 5 minute seg that are above the 70th percentile then ##srcd is a  big conf in child dev, virtual, next april. this project figure out 
# ##predicting to outcomes, which of these factors if anything is a better predictor of CDI and RT 
# ##linear model of child outcomes as a function of AWC clutser size and cluster number, controling for recording length
# ##if any of those other three pop up controlling for the other things, then they must be adding to the piece of the pie indep.
# ##TUESDAY 11 OCLOCK
# ##I WILL HAVE THE DROP VARIABLE AND WE WILL FIGURE OUT A STORY
# ##These variables are adding new stuff to the story?? 
# ##what is more predictive? Clusters more predictive for CDI
# ## further analysis
# 
# 
# ```{r, echo = FALSE, message = FALSE}
# ggplot(clusterAndDistrubtionMeasures, aes(avgHourlyPropDrop, clean_mean_awc))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(avgHourlyPropDrop, clean_sd_awc))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(avgHourlyPropDrop, CDI_18C18C_WG))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(avgHourlyPropDrop, RT18A18C_NO1RT))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# 
# ggplot(clusterAndDistrubtionMeasures, aes(avgHourlyPropDrop, ACC_1818))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# cor.test(clusterAndDistrubtionMeasures$avgHourlyPropDrop, clusterAndDistrubtionMeasures$CDI_18C18C_WG, method = c("pearson"))
# cor.test(clusterAndDistrubtionMeasures$avgHourlyPropDrop, clusterAndDistrubtionMeasures$RT18A18C_NO1RT, method = c("pearson"))
# cor.test(clusterAndDistrubtionMeasures$avgHourlyPropDrop, clusterAndDistrubtionMeasures$ACC_1818, method = c("pearson"))
# 
# 
# 
# 
# 
# ```
# 
# 
# 
# 
# ```{r, echo = FALSE, message = FALSE}
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_mean_awc))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, clean_sd_awc))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# clusterXCDI <- ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, CDI_18C18C_WG))+
#   geom_point() +
#   geom_smooth(method = "lm") +
#    ggtitle("Predicting to Vocabulary at 18 Months") +
#   xlab("Clusters per Hour at 16 Months") + ylab("CDI score at 18 Months")
# 
# ggsave("clusterXCDI.png",dpi=600)
# 
# 
# 
# clusterXACC <- ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, ACC_1818))+
#   geom_point() +
#    geom_smooth(method = "lm")+ 
#   ggtitle("Predicting to Accuracy at 18 Months") +
#   xlab("Clusters per Hour at 16 Months") + ylab("Acurracy at 18 Months")
# 
# grid.arrange(clusterXACC, clusterXCDI, ncol=2)
# 
# ggsave("awesomeGraphs.JPEG", dpi=600)
# 
# 
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, RT18A18C_NO1RT))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# 
# 
# 
# 
# ##predicting to cdi

clusterAndDistrubtionMeasuresNoZeros <- clusterAndDistrubtionMeasures %>% 
  filter(avgclustersize != 0)


model1 = lm(ACC_1818 ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
summary(model1)

model2 = lm(ACC_1818 ~ nclusters, data = clusterAndDistrubtionMeasures)
summary(model2)

model3 = lm(ACC_1818 ~ clean_mean_awc + nclusters, data = clusterAndDistrubtionMeasures)
summary(model3)



# 
# ##Predicting to RT 
# model4 = lm(RT18A18C_NO1RT ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model4)
# 
# model5 = lm(RT18A18C_NO1RT ~ clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model5)
# 
# model6 = lm(RT18A18C_NO1RT ~ clean_mean_awc + clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model6)
# 
# #Predicting to ACC
# model4 = lm(ACC_1818 ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model4)
# 
# model5 = lm(ACC_1818 ~ clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model5)
# 
# model6 = lm(ACC_1818 ~ clean_mean_awc + clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model6)
# 
# 
# 
# ggplot(clusterAndDistrubtionMeasures, aes(clusterPerHr, avgHourlyPropDrop))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# ```
# 
# ```{r, echo = FALSE, message = FALSE}
# ggplot(clusterAndDistrubtionMeasures, aes(maxPropDrop, clean_mean_awc))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(maxPropDrop, clean_sd_awc))+
#   geom_point() +
#    geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(maxPropDrop, ACC_1818))+
#   geom_point() +
#   geom_smooth(method = "lm")
# 
# ggplot(clusterAndDistrubtionMeasures, aes(maxPropDrop, RT18A18C_NO1RT))+
#   geom_point() +
#    geom_smooth(method = "lm")
# ```
# 
# ```{r, echo = FALSE, message = FALSE}
# 
# 
# ##predicting to cdi
# model1 = lm(CDI_18C18C_WG ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model1)
# 
# model2 = lm(CDI_18C18C_WG ~ clean_mean_awc + avgHourlyPropDrop, data = clusterAndDistrubtionMeasures)
# summary(model2)
# 
# 
# model1 = lm(ACC_1818 ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model1)
# 
# model2 = lm(ACC_1818 ~ clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model2)
# 
# model1 = lm(RT18A18C_NO1RT ~ clean_mean_awc , data = clusterAndDistrubtionMeasures)
# summary(model1)
# 
# model2 = lm(RT18A18C_NO1RT ~ clusterPerHr, data = clusterAndDistrubtionMeasures)
# summary(model2)
# 
# mean(TL3Metadata$`MomEdu Mom yrs. Education`)
# 
# 
# 
# sd(clusterAndDistrubtionMeasures$nclusters)
# sd(clusterAndDistrubtionMeasures$avgclustersize)
# sd(clusterAndDistrubtionMeasures$clusterPerHr)
# sd(clusterAndDistrubtionMeasures$clean_mean_awc)
# mean(clusterAndDistrubtionMeasures$nclusters)
# mean(clusterAndDistrubtionMeasures$avgclustersize)
# mean(clusterAndDistrubtionMeasures$clusterPerHr)
# mean(clusterAndDistrubtionMeasures$clean_mean_awc)
# summary(clusterAndDistrubtionMeasures)
# 
```
```{r}


clusterValueNoZero <- clusterValue %>% 
  filter(avgclustersize != 0) 

clusterValuesMinNoZero <- clusterValuesMin %>% 
  filter(avgclustersize != 0) 
  

ggplot( clusterValue$nclusters, clusterValuesMin$nclusters,)

ggplot(distributionPerHour16, aes(clusterValue$nclusters, clusterValuesMin$nclusters))+
  geom_point() +
   geom_smooth(method = "lm")


summary(clusterValueNoZero$avgclustersize)
summary(clusterValuesMinNoZero$avgclustersize)

```

